# âš›ï¸ Quantum Matrix RL

**Quantum Matrix RL** is a reinforcement learning framework that combines symbolic algorithm discovery with quantum-enhanced value function approximation. Inspired by **AlphaTensor** and stabilized using **Quantum Neural Networks (QNNs)**, this system enables agents to discover efficient algorithms for matrix multiplication.

---

## ğŸ§  Project Overview

This project addresses the **Deadly Triad** in reinforcement learning:  
> **Function approximation + bootstrapping + off-policy learning**

We counteract these challenges by using a **quantum-enhanced critic**, trained using off-policy methods in an actor-critic framework. The agent learns symbolic strategies for computing \( C = A 	imes B \) using tensor-based primitives, rather than naive multiplication.

---

## ğŸ§© System Architecture

The system is based on **Quantum Off-Policy Actor-Critic** training with the following components:

- **Environment** (`MatrixMultiplyDiscoveryEnv`): Provides \(A\), \(B\), and tracks \(C_t\)
- **Actor**: Outputs symbolic matrix operations
- **Critic (Quantum QNN)**: Approximates Q-values using variational quantum circuits
- **Replay Buffer**: Stores transitions for off-policy learning

### â¬‡ Architecture Flow

```
[Environment] â†’ (state, reward) â†’ [Actor]
     â†‘                             â†“
     â””â”€â”€ [Replay Buffer] â† action â†â”˜
           â†“                     â†‘
        (state, action, reward, next_state)
                  â†’ [Critic (Quantum QNN)] â†’ Q-value
```

---

## ğŸ§ª Environment: `MatrixMultiplyDiscoveryEnv`

This custom environment simulates symbolic discovery of matrix multiplication.

### ğŸ”¢ Inputs:
- Two matrices \( A \in \mathbb{R}^{n 	imes n} \), \( B \in \mathbb{R}^{n 	imes n} \)
- Flattened into state observations: `[A_flat | B_flat]`

### ğŸ® Actions:
- Each action represents a symbolic operation (e.g., low-rank outer product)
- Action modifies current approximation \(C_t\)

### ğŸ’¸ Reward Function:

\[
r_t = -\|C_t - AB\|_2 - lpha \cdot t - eta \cdot 	ext{op\_cost}_t
\]

- \( \|C_t - AB\|_2 \): Frobenius norm of error
- \( t \): step index (penalizes long algorithms)
- `op_cost`: symbolic or FLOP-based cost for each operation
- \(lpha, eta\): hyperparameters to balance reward components

---

## âœ… Desired Agent Behavior

The optimal RL agent should:
- Approximate \( C = AB \) with minimal symbolic operations
- Prefer low-cost, efficient operations
- Discover strategies that generalize to unseen inputs
- Remain stable during off-policy, bootstrapped training

---

## âš›ï¸ Quantum Critic Design

Quantum Q-value approximation is used to stabilize learning:

- Implemented with **PennyLane** or **Qiskit**
- Parameterized circuits: RX, RY, RZ rotations + CNOT entanglement
- Classical â†’ quantum encodings map \((s, a)\) into circuit observables
- Expectation values output Q-values used in target updates

---

## ğŸ“¦ Folder Structure

```
quantum_matrix_rl/
â”‚
â”œâ”€â”€ main.py                    # Train loop entry
â”œâ”€â”€ config.py                  # Global hyperparameters
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â”‚
â”œâ”€â”€ assets/                    # Diagrams and architecture visualizations
â”‚   â””â”€â”€ architecture.png
â”‚
â”œâ”€â”€ envs/
â”‚   â”œâ”€â”€ matrix_multiply_env.py
â”‚   â””â”€â”€ make_env.py
â”‚
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ actor.py
â”‚   â”œâ”€â”€ critic_classic.py
â”‚   â””â”€â”€ critic_quantum.py
â”‚
â”œâ”€â”€ rl/
â”‚   â”œâ”€â”€ sac_agent.py
â”‚   â”œâ”€â”€ replay_buffer.py
â”‚   â””â”€â”€ trainer.py
â”‚
â”œâ”€â”€ quantum_circuits/
â”‚   â”œâ”€â”€ layers.py
â”‚   â””â”€â”€ encodings.py
â”‚
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ logger.py
â”‚   â”œâ”€â”€ plots.py
â”‚   â””â”€â”€ eval.py
â”‚
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_env.py
â”‚   â”œâ”€â”€ test_critic.py
â”‚   â””â”€â”€ test_agent.py
â”‚
â””â”€â”€ notebooks/
    â”œâ”€â”€ experiment_design.ipynb
    â””â”€â”€ quantum_vs_classic_eval.ipynb
```

---

## ğŸš€ Getting Started

### 1. Install dependencies

```bash
pip install -r requirements.txt
```

Ensure you have:
- Python 3.8+
- PyTorch
- Gym
- PennyLane (or Qiskit)
- NumPy
- Matplotlib
- TensorBoard

---

### 2. Train an Agent

**Train with Classical Critic**:
```bash
python main.py --critic classic --env MatrixMultiplyDiscoveryEnv --steps 100000
```

**Train with Quantum Critic**:
```bash
python main.py --critic quantum --env MatrixMultiplyDiscoveryEnv --steps 100000
```

---

### 3. Visualize Learning

```bash
tensorboard --logdir runs/
```

Or generate reward/error plots manually:

```python
from utils.plots import plot_results
plot_results("runs/quantum/", "Quantum Critic")
```

---

## ğŸ§  Research Directions

- ğŸ§® Discover new matrix multiplication algorithms (like Strassen or beyond)
- ğŸ“Š Compare classical vs quantum critic stability in off-policy RL
- âš¡ Explore FLOP-efficient symbolic decompositions
- ğŸ§© Extend to other linear algebra primitives (e.g., inversion, convolution)

---

## ğŸ“˜ References

- [AlphaTensor: DeepMind, 2022](https://www.nature.com/articles/s41586-022-05172-4)
- [PennyLane Documentation](https://docs.pennylane.ai/)
- [Soft Actor-Critic Paper](https://arxiv.org/abs/1801.01290)
- [RL Book - Sutton & Barto](http://incompleteideas.net/book/the-book-2nd.html)

---

## ğŸªª License

MIT License. Free for academic use, contributions welcome.

---

## ğŸ™Œ Acknowledgments

Special thanks to the open-source community and the researchers who made AlphaTensor and QML frameworks accessible and replicable.

---
